{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fb4ed0d",
   "metadata": {},
   "source": [
    "# Demo Pipeline (Manuscript-aligned)\n",
    "\n",
    "This notebook demonstrates the **end-to-end, evidence-centered QA workflow** described in the manuscript using **synthetic demonstration data**.\n",
    "\n",
    "Pipeline steps:\n",
    "1. Load a small synthetic demonstration corpus (document-level texts + metadata)\n",
    "2. Build a TF–IDF document index and run cosine-similarity retrieval (top-*k*)\n",
    "3. Perform **query-aware extractive summarization** (one sentence per retrieved document; TF–IDF cosine similarity)\n",
    "4. Compute **qualitative uncertainty** (min–max normalize → mean → thresholds → Low/Medium/High)\n",
    "5. (Optional) Run the **demo evaluation** workflow using synthetic CSVs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a960df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Local modules (repository code)\n",
    "from retrieval import build_tfidf_index_from_dataframe, retrieve_top_k\n",
    "from summarization import summarize_retrieved\n",
    "from uncertainty import ConfidenceConfig, compute_overall_confidence_from_retrieval\n",
    "\n",
    "from evaluation import (\n",
    "    parse_relevance_list,\n",
    "    add_precision_at_k,\n",
    "    aggregate_expert_scores,\n",
    "    compute_expert_confidence_majority,\n",
    "    compute_uncertainty_alignment,\n",
    "    build_report_table,\n",
    "    compute_summary_stats,\n",
    "    pairwise_cohens_kappa,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3424ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_ROOT = Path(\"..\").resolve()  # notebook is in notebooks/\n",
    "DATA_ROOT = REPO_ROOT / \"data\"\n",
    "DEMO_CORPUS_DIR = DATA_ROOT / \"demo_corpus\"\n",
    "DEMO_EVAL_DIR = DATA_ROOT / \"demo_evaluation\"\n",
    "\n",
    "DEMO_CORPUS_DIR, DEMO_EVAL_DIR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31a71ab",
   "metadata": {},
   "source": [
    "## 1) Load scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5256da",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios = pd.read_csv(DEMO_EVAL_DIR / \"scenarios.csv\")\n",
    "scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a73f9f",
   "metadata": {},
   "source": [
    "## 2) Load synthetic demonstration corpus\n",
    "\n",
    "This repo stores simplified/synthetic study text files to avoid redistributing copyrighted full texts.\n",
    "We assemble a small document table with:\n",
    "- `doc_id`\n",
    "- `title`\n",
    "- `text` (synthetic)\n",
    "- optional metadata columns (if present)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9675fb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map scenario_id -> demo text filename (convention used in this repo)\n",
    "# Adjust here if your filenames differ.\n",
    "scenario_to_file = {\n",
    "    \"S1\": \"pm25_copd_example.txt\",\n",
    "    \"S2\": \"no2_asthma_example.txt\",\n",
    "    \"S3\": \"lead_neurodevelopment_example.txt\",\n",
    "    \"S4\": \"o3_mortality_example.txt\",\n",
    "    \"S5\": \"vocs_respiratory_example.txt\",\n",
    "    \"S6\": \"cadmium_cvd_example.txt\",\n",
    "}\n",
    "\n",
    "docs = []\n",
    "for sid, fname in scenario_to_file.items():\n",
    "    fpath = DEMO_CORPUS_DIR / fname\n",
    "    text = fpath.read_text(encoding=\"utf-8\")\n",
    "    row = scenarios.loc[scenarios[\"scenario_id\"] == sid].iloc[0].to_dict()\n",
    "    docs.append({\n",
    "        \"doc_id\": sid,                 # keep doc_id aligned with scenario_id for demo clarity\n",
    "        \"title\": f\"{row['pollutant']} → {row['outcome']} (demo study)\",\n",
    "        \"text\": text,\n",
    "        \"pollutant\": row[\"pollutant\"],\n",
    "        \"outcome\": row[\"outcome\"],\n",
    "        \"scenario_id\": sid,\n",
    "    })\n",
    "\n",
    "doc_df = pd.DataFrame(docs)\n",
    "doc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9504b093",
   "metadata": {},
   "source": [
    "## 3) Build TF–IDF index (document-level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13954c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manuscript alignment: document text is formed by concatenating title + abstract/key findings.\n",
    "# In the demo corpus, the synthetic file text already contains structured study information.\n",
    "\n",
    "index = build_tfidf_index_from_dataframe(\n",
    "    doc_df,\n",
    "    doc_id_col=\"doc_id\",\n",
    "    title_col=\"title\",\n",
    "    text_col=\"text\",\n",
    "    # If your helper supports specifying text construction, keep defaults consistent with your implementation.\n",
    ")\n",
    "index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970a3c66",
   "metadata": {},
   "source": [
    "## 4) Run the QA pipeline for a single scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863664de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a scenario to demo\n",
    "sid = \"S1\"\n",
    "query = scenarios.loc[scenarios[\"scenario_id\"] == sid, \"query\"].iloc[0]\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6642102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieval (top-k)\n",
    "k = 3\n",
    "retrieved = retrieve_top_k(query, index, k=k, include_text=True)\n",
    "retrieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961f34dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarization (one sentence per retrieved document; TF–IDF cosine similarity)\n",
    "summary_text, trace_df = summarize_retrieved(query, retrieved)\n",
    "summary_text, trace_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4df4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncertainty (manuscript-aligned): min–max normalize → mean → thresholds (t1, t2)\n",
    "conf_cfg = ConfidenceConfig(normalize=True, t1=0.33, t2=0.67)\n",
    "support, conf_label = compute_overall_confidence_from_retrieval(retrieved, config=conf_cfg)\n",
    "support, conf_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67197618",
   "metadata": {},
   "source": [
    "## 5) Run the pipeline across all demo scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e2b50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for _, r in scenarios.iterrows():\n",
    "    q = r[\"query\"]\n",
    "    sid = r[\"scenario_id\"]\n",
    "\n",
    "    retrieved = retrieve_top_k(q, index, k=3, include_text=True)\n",
    "    summary_text, trace_df = summarize_retrieved(q, retrieved)\n",
    "    support, conf_label = compute_overall_confidence_from_retrieval(retrieved, config=conf_cfg)\n",
    "\n",
    "    rows.append({\n",
    "        \"scenario_id\": sid,\n",
    "        \"query\": q,\n",
    "        \"summary\": summary_text,\n",
    "        \"support_indicator\": support,\n",
    "        \"system_confidence\": conf_label,\n",
    "    })\n",
    "\n",
    "demo_outputs = pd.DataFrame(rows).sort_values(\"scenario_id\")\n",
    "demo_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28205f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export system confidence (demo) if you want the CSV to be generated programmatically\n",
    "out_dir = REPO_ROOT / \"outputs\"\n",
    "out_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "demo_outputs[[\"scenario_id\", \"system_confidence\"]].to_csv(out_dir / \"system_confidence_generated.csv\", index=False)\n",
    "demo_outputs.to_csv(out_dir / \"demo_pipeline_outputs.csv\", index=False)\n",
    "\n",
    "out_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847035de",
   "metadata": {},
   "source": [
    "## 6) Demo evaluation workflow (synthetic ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f17060",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_conf = pd.read_csv(DEMO_EVAL_DIR / \"system_confidence.csv\")\n",
    "expert_ratings = pd.read_csv(DEMO_EVAL_DIR / \"expert_ratings.csv\")\n",
    "retrieval_rel = pd.read_csv(DEMO_EVAL_DIR / \"retrieval_relevance.csv\")\n",
    "\n",
    "system_conf, expert_ratings.head(), retrieval_rel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78174f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse relevance_list strings like \"1|0|1\" into list[int], then compute precision@k\n",
    "rel = retrieval_rel.copy()\n",
    "rel[\"relevance_list\"] = rel[\"relevance_list\"].apply(parse_relevance_list)\n",
    "rel = add_precision_at_k(rel, relevance_col=\"relevance_list\", out_col=\"precision_at_k\")\n",
    "rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c102d12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate expert factuality & interpretability (means per scenario)\n",
    "agg_scores = aggregate_expert_scores(expert_ratings)\n",
    "agg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a149e370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expert majority confidence\n",
    "expert_maj = compute_expert_confidence_majority(expert_ratings)\n",
    "expert_maj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf5794c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System–expert uncertainty alignment (0/1)\n",
    "ua = compute_uncertainty_alignment(\n",
    "    system_confidence_df=system_conf,\n",
    "    expert_majority_df=expert_maj,\n",
    ")\n",
    "ua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24d1ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge to build a Table-1-like report table\n",
    "results = (\n",
    "    scenarios\n",
    "    .merge(rel[[\"scenario_id\", \"k\", \"precision_at_k\"]], on=\"scenario_id\", how=\"left\")\n",
    "    .merge(agg_scores, on=\"scenario_id\", how=\"left\")\n",
    "    .merge(system_conf, on=\"scenario_id\", how=\"left\")\n",
    "    .merge(expert_maj, on=\"scenario_id\", how=\"left\")\n",
    "    .merge(ua[[\"scenario_id\", \"aligned\"]], on=\"scenario_id\", how=\"left\")\n",
    ")\n",
    "\n",
    "report = build_report_table(results)\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1935bcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inter-rater agreement (pairwise Cohen's κ)\n",
    "# Note: κ is most appropriate for categorical labels; here we compute κ separately for factuality and interpretability.\n",
    "kappa_f = pairwise_cohens_kappa(expert_ratings, label_col=\"factuality\")\n",
    "kappa_i = pairwise_cohens_kappa(expert_ratings, label_col=\"interpretability\")\n",
    "kappa_f, kappa_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7da87fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary stats (mean ± SD) and alignment rate, with optional κ means\n",
    "summary = compute_summary_stats(\n",
    "    results=results.merge(ua, on=\"scenario_id\", how=\"left\"),\n",
    "    kappa_factuality=kappa_f,\n",
    "    kappa_interpretability=kappa_i,\n",
    "    precision_col=\"precision_at_k\",\n",
    "    factuality_col=\"factuality_mean\",\n",
    "    interpretability_col=\"interpretability_mean\",\n",
    "    alignment_col=\"aligned\",\n",
    ")\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dfdb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export evaluation outputs (optional)\n",
    "from evaluation import export_outputs\n",
    "\n",
    "export_outputs(\n",
    "    out_dir=out_dir,\n",
    "    report_table=report,\n",
    "    summary_stats=summary,\n",
    "    kappa_factuality=kappa_f,\n",
    "    kappa_interpretability=kappa_i,\n",
    ")\n",
    "\n",
    "sorted(p.name for p in out_dir.glob(\"*.csv\"))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
