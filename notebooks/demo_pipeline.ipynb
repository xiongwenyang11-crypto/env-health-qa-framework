{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9fbbb26",
   "metadata": {},
   "source": [
    "# Demo Pipeline: Query → Retrieval → Extractive Summary → Uncertainty → Answer\n",
    "\n",
    "This notebook demonstrates the end-to-end proof-of-concept pipeline using a small synthetic demonstration corpus.\n",
    "\n",
    "Pipeline:\n",
    "1) Load demo corpus (`data/demo_corpus/`)\n",
    "2) Build TF–IDF index\n",
    "3) Retrieve top-k evidence items (cosine similarity)\n",
    "4) Produce a query-aware extractive summary\n",
    "5) Calibrate and label uncertainty (Low / Medium / High)\n",
    "6) Generate a citation-grounded answer with traceable evidence\n",
    "\n",
    "Note: The demo corpus is synthetic/simplified and does not represent the full literature set used in the study.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa34d290",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1 Imports & Path Setup\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# --- Import repository modules (final, no in-notebook algorithms) ---\n",
    "from retrieval.tfidf_index import build_tfidf_index\n",
    "from retrieval.cosine_search import retrieve_top_k\n",
    "\n",
    "from summarization import summarize_retrieved\n",
    "from uncertainty import annotate_retrieval_df, overall_confidence, ConfidenceConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b66d3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2 Load Demo Corpus\n",
    "REPO_ROOT = Path(\"..\")  # notebooks/ is one level below repo root\n",
    "CORPUS_DIR = REPO_ROOT / \"data\" / \"demo_corpus\"\n",
    "\n",
    "assert CORPUS_DIR.exists(), f\"Corpus folder not found: {CORPUS_DIR.resolve()}\"\n",
    "\n",
    "@dataclass\n",
    "class Doc:\n",
    "    doc_id: str\n",
    "    title: str\n",
    "    text: str\n",
    "\n",
    "def load_demo_corpus(corpus_dir: Path) -> List[Doc]:\n",
    "    docs: List[Doc] = []\n",
    "    for fp in sorted(corpus_dir.glob(\"*.txt\")):\n",
    "        raw = fp.read_text(encoding=\"utf-8\").strip()\n",
    "        doc_id = fp.stem\n",
    "        title = fp.stem.replace(\"_\", \" \").title()\n",
    "        docs.append(Doc(doc_id=doc_id, title=title, text=raw))\n",
    "    if not docs:\n",
    "        raise RuntimeError(f\"No .txt files found under: {corpus_dir.resolve()}\")\n",
    "    return docs\n",
    "\n",
    "docs = load_demo_corpus(CORPUS_DIR)\n",
    "len(docs), [d.doc_id for d in docs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f61fc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3 Build TF–IDF index\n",
    "texts = [d.text for d in docs]\n",
    "doc_ids = [d.doc_id for d in docs]\n",
    "titles = [d.title for d in docs]\n",
    "\n",
    "index = build_tfidf_index(\n",
    "    texts=texts,\n",
    "    doc_ids=doc_ids,\n",
    "    titles=titles,\n",
    "    ngram_range=(1, 2),\n",
    "    max_features=5000,\n",
    "    stop_words=\"english\",\n",
    ")\n",
    "\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f4f99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4 Define a query & retrieve top-k evidence\n",
    "## Retrieval demo\n",
    "\n",
    "We retrieve the top-k evidence items using TF–IDF + cosine similarity.  \n",
    "The retrieval output includes:\n",
    "- similarity score\n",
    "- a short snippet for readability\n",
    "- the full text (`text`) for downstream summarization (demo corpus only)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee9317b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5 Retrieval\n",
    "query = \"Does long-term PM2.5 exposure increase COPD exacerbations?\"\n",
    "\n",
    "retrieved = retrieve_top_k(\n",
    "    query=query,\n",
    "    index=index,\n",
    "    k=3,\n",
    "    include_text=True,   # ensures summarization can use `text`\n",
    "    snippet_len=360\n",
    ")\n",
    "\n",
    "retrieved[[\"rank\", \"doc_id\", \"similarity\", \"title\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc79ead7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6 Show evidence cards (snippet view)\n",
    "for _, r in retrieved.iterrows():\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(f\"Evidence Card #{int(r['rank'])}\")\n",
    "    print(f\"Title: {r['title']}\")\n",
    "    print(f\"doc_id: {r['doc_id']}\")\n",
    "    print(f\"Similarity: {r['similarity']:.4f}\")\n",
    "    print(f\"Snippet: {r['snippet']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46a6e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 7 Uncertainty calibration (annotate retrieval table)\n",
    "## Uncertainty calibration\n",
    "\n",
    "We calibrate similarity scores and map them to qualitative confidence labels:\n",
    "- Low / Medium / High\n",
    "\n",
    "This step produces:\n",
    "- calibrated_score\n",
    "- confidence label per evidence item\n",
    "- an overall confidence label for the final answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee1f063",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 8 Uncertainty labeling\n",
    "# You can keep defaults or set the same parameters used in the manuscript\n",
    "cfg = ConfidenceConfig(\n",
    "    normalize=True,\n",
    "    a=10.0,\n",
    "    b=0.5,\n",
    "    t_low=0.33,\n",
    "    t_high=0.67\n",
    ")\n",
    "\n",
    "retrieved_u = annotate_retrieval_df(\n",
    "    retrieved,\n",
    "    similarity_col=\"similarity\",\n",
    "    out_score_col=\"calibrated_score\",\n",
    "    out_label_col=\"confidence\",\n",
    "    config=cfg\n",
    ")\n",
    "\n",
    "retrieved_u[[\"rank\", \"doc_id\", \"similarity\", \"calibrated_score\", \"confidence\", \"title\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16e9e24",
   "metadata": {},
   "source": [
    "## Extractive summarization\n",
    "\n",
    "We generate a query-aware extractive summary by selecting top sentences from the retrieved evidence texts.\n",
    "\n",
    "Outputs:\n",
    "- summary_text: concatenated top sentences\n",
    "- selected_sentences: traceable sentence-level evidence (doc_id, title, similarity, sentence, sent_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be27324",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 10 Summarization\n",
    "summary_text, selected_sentences = summarize_retrieved(\n",
    "    query=query,\n",
    "    retrieved_df=retrieved_u,\n",
    "    top_n_sentences=3\n",
    ")\n",
    "\n",
    "summary_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756878d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 11) Inspect selected sentences (traceability)\n",
    "selected_sentences[[\"doc_id\", \"sent_score\", \"similarity\", \"sentence\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f28b3d",
   "metadata": {},
   "source": [
    "## Final answer\n",
    "\n",
    "We present:\n",
    "- overall confidence (derived from evidence confidence labels)\n",
    "- evidence-grounded summary\n",
    "- traceable supporting evidence list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9ea70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 13) Final answer generation\n",
    "conf = overall_confidence(retrieved_u, label_col=\"confidence\", rank_col=\"rank\", method=\"top1\")\n",
    "\n",
    "answer_lines = []\n",
    "answer_lines.append(f\"Question: {query}\")\n",
    "answer_lines.append(\"\")\n",
    "answer_lines.append(f\"Answer Summary (Confidence: {conf})\")\n",
    "answer_lines.append(summary_text)\n",
    "answer_lines.append(\"\")\n",
    "answer_lines.append(\"Supporting Evidence\")\n",
    "for _, r in retrieved_u.sort_values(\"rank\").iterrows():\n",
    "    answer_lines.append(\n",
    "        f\"- {r['title']} (doc_id: {r['doc_id']}, similarity: {r['similarity']:.3f}, confidence: {r['confidence']})\"\n",
    "    )\n",
    "\n",
    "print(\"\\n\".join(answer_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98601b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 14) Try additional queries\n",
    "more_queries = [\n",
    "    \"Does short-term NO2 exposure worsen childhood asthma symptoms?\",\n",
    "    \"Does early-life lead exposure affect neurodevelopment?\"\n",
    "]\n",
    "\n",
    "for q in more_queries:\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    r = retrieve_top_k(q, index, k=3, include_text=True, snippet_len=360)\n",
    "    r = annotate_retrieval_df(r, config=cfg)\n",
    "    s, sel = summarize_retrieved(q, r, top_n_sentences=3)\n",
    "    c = overall_confidence(r, method=\"top1\")\n",
    "\n",
    "    print(f\"Question: {q}\\n\")\n",
    "    print(f\"Answer Summary (Confidence: {c})\")\n",
    "    print(s)\n",
    "    print(\"\\nSupporting Evidence\")\n",
    "    for _, row in r.sort_values(\"rank\").iterrows():\n",
    "        print(f\"- {row['title']} (doc_id: {row['doc_id']}, confidence: {row['confidence']}, sim: {row['similarity']:.3f})\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
