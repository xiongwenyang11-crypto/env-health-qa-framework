{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "916180e1",
   "metadata": {},
   "source": [
    "# Evaluation Demo (Module-based): Precision@k, Expert Ratings, Uncertainty Alignment, and Agreement\n",
    "\n",
    "This notebook demonstrates the evaluation workflow using CSV-based synthetic demonstration data.\n",
    "\n",
    "It:\n",
    "1) Loads evaluation CSVs under `data/demo_evaluation/`\n",
    "2) Computes retrieval metrics (citation precision@k)\n",
    "3) Aggregates expert ratings (factuality, interpretability)\n",
    "4) Computes inter-rater agreement (pairwise Cohen's κ)\n",
    "5) Computes uncertainty alignment (system vs expert majority vote)\n",
    "6) Produces a Table-1-like report table and summary statistics\n",
    "7) Optionally exports outputs to `outputs/`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b897a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1) Imports\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# --- evaluation modules (core logic lives in /evaluation, notebook just calls) ---\n",
    "from evaluation import (\n",
    "    parse_relevance_list,\n",
    "    add_precision_at_k,\n",
    "    pairwise_cohens_kappa,\n",
    "    aggregate_expert_scores,\n",
    "    compute_expert_confidence_majority,\n",
    "    compute_uncertainty_alignment,\n",
    "    build_report_table,\n",
    "    compute_summary_stats,\n",
    "    export_outputs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa0704f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2)  Load CSV demo data\n",
    "REPO_ROOT = Path(\"..\")  # notebooks/ is one level below repo root\n",
    "EVAL_DIR = REPO_ROOT / \"data\" / \"demo_evaluation\"\n",
    "assert EVAL_DIR.exists(), f\"demo_evaluation folder not found: {EVAL_DIR.resolve()}\"\n",
    "\n",
    "scenarios = pd.read_csv(EVAL_DIR / \"scenarios.csv\")\n",
    "retrieval_relevance = pd.read_csv(EVAL_DIR / \"retrieval_relevance.csv\")\n",
    "expert_ratings = pd.read_csv(EVAL_DIR / \"expert_ratings.csv\")\n",
    "system_confidence = pd.read_csv(EVAL_DIR / \"system_confidence.csv\")\n",
    "\n",
    "# Parse relevance_list: \"1|0|1\" -> [1,0,1]\n",
    "retrieval_relevance[\"relevance_list\"] = retrieval_relevance[\"relevance_list\"].apply(parse_relevance_list)\n",
    "\n",
    "scenarios.head(), retrieval_relevance.head(), expert_ratings.head(), system_confidence.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f49cdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def require_columns(df: pd.DataFrame, required: list[str], df_name: str) -> None:\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(\n",
    "            f\"[SchemaError] {df_name} is missing columns: {missing}\\n\"\n",
    "            f\"Available columns: {list(df.columns)}\"\n",
    "        )\n",
    "\n",
    "# --- Schema checks (fail fast, avoid silent errors) ---\n",
    "require_columns(\n",
    "    scenarios,\n",
    "    [\"scenario_id\", \"pollutant\", \"outcome\", \"query\"],\n",
    "    \"scenarios.csv\",\n",
    ")\n",
    "\n",
    "require_columns(\n",
    "    retrieval_relevance,\n",
    "    [\"scenario_id\", \"k\", \"relevance_list\"],\n",
    "    \"retrieval_relevance.csv\",\n",
    ")\n",
    "\n",
    "require_columns(\n",
    "    expert_ratings,\n",
    "    [\"scenario_id\", \"expert\", \"factuality\", \"interpretability\", \"expert_confidence\"],\n",
    "    \"expert_ratings.csv\",\n",
    ")\n",
    "\n",
    "require_columns(\n",
    "    system_confidence,\n",
    "    [\"scenario_id\", \"system_confidence\"],\n",
    "    \"system_confidence.csv\",\n",
    ")\n",
    "\n",
    "print(\"Schema check passed ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dec161",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3) Compute citation precision@k\n",
    "retrieval_relevance = add_precision_at_k(\n",
    "    retrieval_relevance,\n",
    "    relevance_col=\"relevance_list\",\n",
    "    out_col=\"precision_at_k\",\n",
    ")\n",
    "\n",
    "retrieval_relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3e415b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4) Merge retrieval results with scenarios\n",
    "results = scenarios.merge(\n",
    "    retrieval_relevance[[\"scenario_id\", \"k\", \"precision_at_k\"]],\n",
    "    on=\"scenario_id\",\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092fe64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5) Aggregate expert ratings per scenario\n",
    "expert_agg = aggregate_expert_scores(\n",
    "    expert_ratings,\n",
    "    scenario_col=\"scenario_id\",\n",
    "    factuality_col=\"factuality\",\n",
    "    interpretability_col=\"interpretability\",\n",
    ")\n",
    "\n",
    "expert_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a642308",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6) Add system confidence labels\n",
    "results = results.merge(expert_agg, on=\"scenario_id\", how=\"left\")\n",
    "results = results.merge(system_confidence, on=\"scenario_id\", how=\"left\")\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfab8798",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 7) Inter-rater agreement: pairwise Cohen’s κ\n",
    "kappa_factuality = pairwise_cohens_kappa(expert_ratings, label_col=\"factuality\", scenario_col=\"scenario_id\", expert_col=\"expert\")\n",
    "kappa_interpretability = pairwise_cohens_kappa(expert_ratings, label_col=\"interpretability\", scenario_col=\"scenario_id\", expert_col=\"expert\")\n",
    "\n",
    "kappa_factuality, kappa_interpretability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad00492d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 8) Uncertainty alignment: system vs expert majority vote\n",
    "expert_conf_majority = compute_expert_confidence_majority(\n",
    "    expert_ratings,\n",
    "    scenario_col=\"scenario_id\",\n",
    "    conf_col=\"expert_confidence\",\n",
    ")\n",
    "\n",
    "ua = compute_uncertainty_alignment(\n",
    "    system_confidence_df=system_confidence,\n",
    "    expert_majority_df=expert_conf_majority,\n",
    "    scenario_col=\"scenario_id\",\n",
    "    system_conf_col=\"system_confidence\",\n",
    "    expert_conf_col=\"expert_confidence_majority\",\n",
    ")\n",
    "\n",
    "expert_conf_majority, ua\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa61ce4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 9) Combine uncertainty alignment into results\n",
    "results = results.merge(\n",
    "    ua[[\"scenario_id\", \"expert_confidence_majority\", \"aligned\"]],\n",
    "    on=\"scenario_id\",\n",
    "    how=\"left\",\n",
    ")\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e19d466",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 10) Build Table-1-like report table\n",
    "report_table = build_report_table(results)\n",
    "report_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d593107",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 11) Summary statistics (mean ± SD + alignment rate + κ means)\n",
    "summary_stats = compute_summary_stats(\n",
    "    results=results,\n",
    "    kappa_factuality=kappa_factuality,\n",
    "    kappa_interpretability=kappa_interpretability,\n",
    "    precision_col=\"precision_at_k\",\n",
    "    factuality_col=\"factuality_mean\",\n",
    "    interpretability_col=\"interpretability_mean\",\n",
    "    alignment_col=\"aligned\",\n",
    ")\n",
    "\n",
    "summary_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8775f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 12) Code cell — Export outputs\n",
    "OUT_DIR = REPO_ROOT / \"outputs\"\n",
    "export_outputs(\n",
    "    out_dir=OUT_DIR,\n",
    "    report_table=report_table,\n",
    "    summary_stats=summary_stats,\n",
    "    kappa_factuality=kappa_factuality,\n",
    "    kappa_interpretability=kappa_interpretability,\n",
    ")\n",
    "print(\"Exported outputs to:\", OUT_DIR.resolve())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
