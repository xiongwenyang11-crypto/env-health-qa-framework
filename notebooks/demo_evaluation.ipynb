{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a3b5b27",
   "metadata": {},
   "source": [
    "# Demo Evaluation (Manuscript-aligned)\n",
    "\n",
    "This notebook demonstrates the **evaluation workflow** for the interpretable, evidence-centered QA framework using the repository’s **synthetic demonstration CSVs**.\n",
    "\n",
    "**Important:** The CSVs in `data/demo_evaluation/` are **synthetic** and provided to reproduce the *logic* of the evaluation pipeline (schema + metric computation). They **do not** represent real study outputs or real expert ratings.\n",
    "\n",
    "Evaluation dimensions shown here mirror the manuscript:\n",
    "- **Citation precision@k** (k=3 in the demo)\n",
    "- **Factual consistency / factuality** (ordinal in the manuscript; demo may use a simplified scale)\n",
    "- **Interpretability** (Likert-style ordinal ratings)\n",
    "- **Uncertainty alignment** between system confidence labels and expert consensus confidence\n",
    "- **Inter-rater agreement** among experts (pairwise Cohen’s κ)\n",
    "\n",
    "We also compute **weighted Cohen’s κ (linear weights)** for uncertainty alignment (Low/Medium/High),\n",
    "matching the manuscript’s reporting approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fa06c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "# Repo evaluation utilities\n",
    "from evaluation import (\n",
    "    parse_relevance_list,\n",
    "    add_precision_at_k,\n",
    "    aggregate_expert_scores,\n",
    "    compute_expert_confidence_majority,\n",
    "    compute_uncertainty_alignment,\n",
    "    build_report_table,\n",
    "    compute_summary_stats,\n",
    "    pairwise_cohens_kappa,\n",
    "    export_outputs,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d375fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_ROOT = Path(\"..\").resolve()  # notebook is in notebooks/\n",
    "DATA_ROOT = REPO_ROOT / \"data\"\n",
    "DEMO_EVAL_DIR = DATA_ROOT / \"demo_evaluation\"\n",
    "OUT_DIR = REPO_ROOT / \"outputs\"\n",
    "OUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "DEMO_EVAL_DIR, OUT_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79241894",
   "metadata": {},
   "source": [
    "## 1) Load synthetic evaluation tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4b7594",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios = pd.read_csv(DEMO_EVAL_DIR / \"scenarios.csv\")\n",
    "system_conf = pd.read_csv(DEMO_EVAL_DIR / \"system_confidence.csv\")\n",
    "expert_ratings = pd.read_csv(DEMO_EVAL_DIR / \"expert_ratings.csv\")\n",
    "retrieval_rel = pd.read_csv(DEMO_EVAL_DIR / \"retrieval_relevance.csv\")\n",
    "\n",
    "scenarios, system_conf.head(), expert_ratings.head(), retrieval_rel.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6879b133",
   "metadata": {},
   "source": [
    "## 2) Retrieval evaluation: citation precision@k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250fc92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel = retrieval_rel.copy()\n",
    "rel[\"relevance_list\"] = rel[\"relevance_list\"].apply(parse_relevance_list)\n",
    "rel = add_precision_at_k(rel, relevance_col=\"relevance_list\", out_col=\"precision_at_k\")\n",
    "rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be5e462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive summary (demo)\n",
    "rel[\"precision_at_k\"].mean(), rel[\"precision_at_k\"].std(ddof=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d00cbc3",
   "metadata": {},
   "source": [
    "## 3) Expert aggregation: factuality & interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d73403",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_scores = aggregate_expert_scores(expert_ratings)\n",
    "agg_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadd8b0a",
   "metadata": {},
   "source": [
    "## 4) Expert consensus confidence (majority vote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24674caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_maj = compute_expert_confidence_majority(expert_ratings)\n",
    "expert_maj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e228d6e0",
   "metadata": {},
   "source": [
    "## 5) System–expert uncertainty alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429246ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "ua = compute_uncertainty_alignment(\n",
    "    system_confidence_df=system_conf,\n",
    "    expert_majority_df=expert_maj,\n",
    ")\n",
    "ua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f36b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exact agreement rate (demo alignment signal)\n",
    "ua[\"aligned\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7f5422",
   "metadata": {},
   "source": [
    "### Weighted Cohen’s κ (linear weights) for uncertainty alignment\n",
    "\n",
    "To match manuscript-style reporting, we compute weighted κ between:\n",
    "- system confidence labels (Low/Medium/High)\n",
    "- expert majority confidence labels (Low/Medium/High)\n",
    "\n",
    "This is interpreted as *interpretability alignment* rather than probabilistic calibration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b56f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map ordered labels to integers for weighted kappa\n",
    "order = {\"Low\": 0, \"Medium\": 1, \"High\": 2}\n",
    "\n",
    "tmp = ua.dropna(subset=[\"system_confidence\", \"expert_confidence_majority\"]).copy()\n",
    "sys_y = tmp[\"system_confidence\"].map(order).astype(int)\n",
    "exp_y = tmp[\"expert_confidence_majority\"].map(order).astype(int)\n",
    "\n",
    "kappa_weighted = cohen_kappa_score(sys_y, exp_y, weights=\"linear\")\n",
    "kappa_weighted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25348a89",
   "metadata": {},
   "source": [
    "## 6) Build a report table (Table-2-like)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746d561e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = (\n",
    "    scenarios\n",
    "    .merge(rel[[\"scenario_id\", \"k\", \"precision_at_k\"]], on=\"scenario_id\", how=\"left\")\n",
    "    .merge(agg_scores, on=\"scenario_id\", how=\"left\")\n",
    "    .merge(system_conf, on=\"scenario_id\", how=\"left\")\n",
    "    .merge(expert_maj, on=\"scenario_id\", how=\"left\")\n",
    "    .merge(ua[[\"scenario_id\", \"aligned\"]], on=\"scenario_id\", how=\"left\")\n",
    ")\n",
    "\n",
    "report = build_report_table(results)\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09810b9",
   "metadata": {},
   "source": [
    "## 7) Inter-rater agreement among experts (pairwise Cohen’s κ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418986dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa_f = pairwise_cohens_kappa(expert_ratings, label_col=\"factuality\")\n",
    "kappa_i = pairwise_cohens_kappa(expert_ratings, label_col=\"interpretability\")\n",
    "\n",
    "kappa_f, kappa_i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd521fa3",
   "metadata": {},
   "source": [
    "## 8) Summary statistics (mean ± SD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa610a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = compute_summary_stats(\n",
    "    results=results.merge(ua, on=\"scenario_id\", how=\"left\"),\n",
    "    kappa_factuality=kappa_f,\n",
    "    kappa_interpretability=kappa_i,\n",
    "    precision_col=\"precision_at_k\",\n",
    "    factuality_col=\"factuality_mean\",\n",
    "    interpretability_col=\"interpretability_mean\",\n",
    "    alignment_col=\"aligned\",\n",
    ")\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b126ab5",
   "metadata": {},
   "source": [
    "## 9) Export outputs (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b4eb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_outputs(\n",
    "    out_dir=OUT_DIR,\n",
    "    report_table=report,\n",
    "    summary_stats=summary,\n",
    "    kappa_factuality=kappa_f,\n",
    "    kappa_interpretability=kappa_i,\n",
    ")\n",
    "\n",
    "sorted(p.name for p in OUT_DIR.glob(\"demo_*.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78184a9d",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "- The demonstration CSVs may use simplified scales for illustration.\n",
    "  The evaluation utilities are schema-driven and can be reused with real evaluation tables\n",
    "  as long as the column names and coding conventions are consistent.\n",
    "- For manuscript-style uncertainty alignment reporting, this notebook computes weighted κ\n",
    "  (linear weights) using the expert majority confidence labels.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
